Title: BOLD: Bayesian Online Liar Detector via Ethereum Smart Contract Mechanism

CHECKLIST!!

[1] supervise learning - non- deep learning
- Understand the LIAR dataset
- read the Wang paper- liar liar pants on fire
- fix the jenky sklearn code
- get the appropriate results for acc, f1-score, prediction etc
- receive better results, eg: graphs, distribution, convolution matrix

[2] web framework
- create a flask based site
- setup SQL
- get/post command
- get good results (showing that the algorithm is learning etc)

[3] BERT ALGORITHM
- create BERT (bayesian deep NN) alg (pseudocode)??
- implement the BERT alg
- get highest qulaity acc on LIAR dataset ---- this is the most important result!!!
- define the alg and the architecture
- math model be defined

[4] BOLD archtiecture
- create a novel arch. that comprises ethereum smart contract
with BDNN alg
- sample implementation result
- BOLD arch. diagram is required
- future work in the area
#############################################################################
 ####################################################################
 BERT: Bidirectional Encoder Representation Transformer
 
 <def>: BERT is a deep neural network that uses pre-train bidrectional representation from unlabel text to perform language 
 related task, such as: Q&A, language inference, etc. It is fine-tuned ( ).
     : BERT = bidirectional (ELMO) + transformer (GPT)
 
 <property>: 
      + Contextual: captures the context of the sentence with bidirectional property
      + Pre- train model: a model that was trained on large dataset to solve a problem similar to the one specified to solve.
      it captures the universal representation of langauge.
      + semi- supervise learning: train step, in which a small lable example and large number of unlabeled examples.
      + fine- tuned: transfer the learn knowledge from train to target test.
      + Bidirectional: fuse the left and right language model/ bi means both direction
      + Encoder: converts input token to an embedding
      + Transformer: an architecture aims to solve sequence-to-sequence (models in NLP are
      used to convert sequences of type A to type B. for instance: translate language from
      one to another) task while handling long range dependencies with ease
      + BERT-base: 12 layer, 768 hidden, 12 head
      + BERT-large: 24 layer, 1024 hidden, 16 head

<Transformer>:
      + multi-head self attention: models context
      + feed forward layer: compute non linear hierarchial features
      + layer norm and residuals: makes train deep netwroks healthy
      + positional embeddings: allows model to learn raltive positioning
      
<diff: PTM v transfer learning>: one is a model, the other is learning mechanism. both are quite similar.

Q. what is the advantage of transformer over LSTM?
A. + slef attention leads to no locality bias. which means long distance context has equal opportunity
   + single maultiplication per layer leads to efficiency on TPU. as effective batch size is num of words not sequences
   
#########################################################################
##########################################################################
#####################


::>
 <def>: [CLS]: classifier token
        [SEP]: sentence seperator
        

 
