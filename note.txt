Title: BOLD: Bayesian Online Liar Detector via Ethereum Smart Contract Mechanism

CHECKLIST!!

[1] supervise learning - non- deep learning
- Understand the LIAR dataset
- read the Wang paper- liar liar pants on fire
- fix the jenky sklearn code
- get the appropriate results for acc, f1-score, prediction etc
- receive better results, eg: graphs, distribution, convolution matrix

[2] web framework
- create a flask based site
- setup SQL
- get/post command
- get good results (showing that the algorithm is learning etc)

[3] BERT ALGORITHM
- create BERT (bayesian deep NN) alg (pseudocode)??
- implement the BERT alg
- get highest qulaity acc on LIAR dataset ---- this is the most important result!!!
- define the alg and the architecture
- math model be defined

[4] BOLD archtiecture
- create a novel arch. that comprises ethereum smart contract
with BDNN alg
- sample implementation result
- BOLD arch. diagram is required
- future work in the area

---------------------THE END-------------------------------
Hello. these are the guiding notes in order to complete the thesis work. 
##please be informed that you require to complete the thesis case and extend I-20 to summer semester. these are hard times. I have seen harder times. rest asure, that we will make it through. <finger cross>
CS7999: program of completion:
Contents:
 - related work - dataset - architecture - results - conclusion
 
 #################################################################
 # The main contribution of the theis is to create a project called 
 # Go- news, which provides a solution for the fake news problem as
 # a symbiotic relationship of ML and blockchain/ smart contract.
 # it wont be easy but this thesis can be treated a final term project
 # porposal rather then a true reaserch endeavour.
 #################################################################
 ####################################################################
 BERT: Bidirectional Encoder Representation Transformer
 
 <def>: BERT is a deep neural network that uses pre-train bidrectional representation from unlabel text to perform language 
 related task, such as: Q&A, language inference, etc. It is fine-tuned ( ).
     : BERT = bidirectional (ELMO) + transformer (GPT)
 
 <property>: 
      + Pre- train model:
      + semi- supervise learning: train step, in which a small lable example and large number of unlabeled examples.
      + fine- tuned: transfer the learn knowledge from train to target test.
      + Bidirectional: fuse the left and right language model/ bi means both direction
      + Encoder:
      + Transformer:

<Transformer>:
      + multi-head self attention: models context
      + feed forward layer: compute non linear hierarchial features
      + layer norm and residuals: makes train deep netwroks healthy
      + positional embeddings: allows model to learn raltive positioning
 
 <def>: [CLS]: classifier token
        [SEP]: sentence seperator
        

 
